{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDyXspY7UC15"
      },
      "source": [
        "# **Checkpoint 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_nMmp7KUGJA"
      },
      "source": [
        "# **Part A**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imSv9XE4egyU"
      },
      "source": [
        "## **Subtask 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2TN92QBTa96"
      },
      "source": [
        "Dataset name + link: https://www.kaggle.com/datasets/omnamahshivai/surgical-dataset-binary-classification\n",
        "\n",
        "License/terms: Published by Anesthesia & Analgesia (Wolters Kluwer / International Anesthesia Research Society) and is not open access. All rights are reserved, and reuse, redistribution, or reproduction beyond personal or fair use requires prior permission from the publisher.\n",
        "\n",
        "Prediction task + target definition: Predict whether a surgery will have a complication or not given features; target: `complication`\n",
        "\n",
        "Intended use / decision context: We would like to discover how well certain biodata features of an individual can be a predictor on surgeries having complications.\n",
        "\n",
        "Feature dictionary (5-10 key features): https://www.causeweb.org/tshs/datasets/Surgery%20Timing%20Data%20Dictionary.pdf\n",
        "* bmi\n",
        "* baseline_cancer\n",
        "* baseline_cvd\n",
        "* baseline_dementia\n",
        "* baseline_diabetes\n",
        "* age\n",
        "* baseline_osteoart\n",
        "* baseline_digestive\n",
        "* baseline_psych\n",
        "* ...\n",
        "\n",
        "Known limitations/risks (2-3 bullets):\n",
        "* The study is not randomized, so it can show association but not causation\n",
        "* Focusing on 30-day mortality may miss longer-term complications or functional outcomes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDeIWYUlUheH",
        "outputId": "178da08f-2d8a-4722-93c1-f8920a7286f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/omnamahshivai/surgical-dataset-binary-classification?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 206k/206k [00:00<00:00, 4.82MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Dataset downloaded to: C:\\Users\\ryans\\.cache\\kagglehub\\datasets\\omnamahshivai\\surgical-dataset-binary-classification\\versions\\1\n",
            "Files: ['Surgical-deepnet.csv']\n",
            "Dataset shape: (14635, 25)\n",
            "     bmi   Age  asa_status  baseline_cancer  baseline_charlson  baseline_cvd  \\\n",
            "0  19.31  59.2           1                1                  0             0   \n",
            "1  18.73  59.1           0                0                  0             0   \n",
            "2  21.85  59.0           0                0                  0             0   \n",
            "3  18.49  59.0           1                0                  1             0   \n",
            "4  19.70  59.0           1                0                  0             0   \n",
            "\n",
            "   baseline_dementia  baseline_diabetes  baseline_digestive  \\\n",
            "0                  0                  0                   0   \n",
            "1                  0                  0                   0   \n",
            "2                  0                  0                   0   \n",
            "3                  0                  1                   1   \n",
            "4                  0                  0                   0   \n",
            "\n",
            "   baseline_osteoart  ...  complication_rsi  dow  gender   hour  month  \\\n",
            "0                  0  ...             -0.57    3       0   7.63      6   \n",
            "1                  0  ...              0.21    0       0  12.93      0   \n",
            "2                  0  ...              0.00    2       0   7.68      5   \n",
            "3                  0  ...             -0.65    2       1   7.58      4   \n",
            "4                  0  ...              0.00    0       0   7.88     11   \n",
            "\n",
            "   moonphase  mort30  mortality_rsi  race  complication  \n",
            "0          1       0          -0.43     1             0  \n",
            "1          1       0          -0.41     1             0  \n",
            "2          3       0           0.08     1             0  \n",
            "3          3       0          -0.32     1             0  \n",
            "4          0       0           0.00     1             0  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "Number of duplicate rows: 2902\n",
            "Sample duplicate rows:\n",
            "Shape after removing duplicates: (11733, 25)\n",
            "Missing values per column:\n",
            "bmi                    0\n",
            "Age                    0\n",
            "asa_status             0\n",
            "baseline_cancer        0\n",
            "baseline_charlson      0\n",
            "baseline_cvd           0\n",
            "baseline_dementia      0\n",
            "baseline_diabetes      0\n",
            "baseline_digestive     0\n",
            "baseline_osteoart      0\n",
            "baseline_psych         0\n",
            "baseline_pulmonary     0\n",
            "ahrq_ccs               0\n",
            "ccsComplicationRate    0\n",
            "ccsMort30Rate          0\n",
            "complication_rsi       0\n",
            "dow                    0\n",
            "gender                 0\n",
            "hour                   0\n",
            "month                  0\n",
            "moonphase              0\n",
            "mort30                 0\n",
            "mortality_rsi          0\n",
            "race                   0\n",
            "complication           0\n",
            "dtype: int64\n",
            "\n",
            "Total missing values in dataset: 0\n",
            "\n",
            "Missing percentage per column:\n",
            "bmi                    0.0\n",
            "Age                    0.0\n",
            "asa_status             0.0\n",
            "baseline_cancer        0.0\n",
            "baseline_charlson      0.0\n",
            "baseline_cvd           0.0\n",
            "baseline_dementia      0.0\n",
            "baseline_diabetes      0.0\n",
            "baseline_digestive     0.0\n",
            "baseline_osteoart      0.0\n",
            "baseline_psych         0.0\n",
            "baseline_pulmonary     0.0\n",
            "ahrq_ccs               0.0\n",
            "ccsComplicationRate    0.0\n",
            "ccsMort30Rate          0.0\n",
            "complication_rsi       0.0\n",
            "dow                    0.0\n",
            "gender                 0.0\n",
            "hour                   0.0\n",
            "month                  0.0\n",
            "moonphase              0.0\n",
            "mort30                 0.0\n",
            "mortality_rsi          0.0\n",
            "race                   0.0\n",
            "complication           0.0\n",
            "dtype: float64\n",
            "\n",
            "No missing values found in the dataset.\n",
            "Sum of complication: 3690\n",
            "Sum of mort30: 58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\n",
        "    \"omnamahshivai/surgical-dataset-binary-classification\"\n",
        ")\n",
        "\n",
        "print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "# List files in the dataset directory\n",
        "files = os.listdir(path)\n",
        "\n",
        "print(\"Files:\", files)\n",
        "\n",
        "# Load the CSV file (update name if needed)\n",
        "csv_file = \"Surgical-deepnet.csv\"  # this is the main file in the dataset\n",
        "csv_path = os.path.join(path, csv_file)\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Basic info\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Check for duplicate rows\n",
        "num_duplicates = df.duplicated().sum()\n",
        "\n",
        "print(\"Number of duplicate rows:\", num_duplicates)\n",
        "\n",
        "# View duplicates\n",
        "if num_duplicates > 0:\n",
        "    duplicates = df[df.duplicated()]\n",
        "\n",
        "    print(\"Sample duplicate rows:\")\n",
        "\n",
        "# Remove duplicates\n",
        "df_no_duplicates = df.drop_duplicates()\n",
        "\n",
        "print(\"Shape after removing duplicates:\", df_no_duplicates.shape)\n",
        "\n",
        "# Check for missing values in each column\n",
        "missing_per_column = df_no_duplicates.isna().sum()\n",
        "\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_per_column)\n",
        "\n",
        "# Total number of missing values in the dataset\n",
        "total_missing = missing_per_column.sum()\n",
        "\n",
        "print(\"\\nTotal missing values in dataset:\", total_missing)\n",
        "\n",
        "# Percentage of missing values per column\n",
        "missing_percentage = (missing_per_column / len(df_no_duplicates)) * 100\n",
        "\n",
        "print(\"\\nMissing percentage per column:\")\n",
        "print(missing_percentage)\n",
        "\n",
        "# Show only columns that actually have missing values\n",
        "columns_with_missing = missing_per_column[missing_per_column > 0]\n",
        "\n",
        "if not columns_with_missing.empty:\n",
        "    print(\"\\nColumns with missing values:\")\n",
        "    print(columns_with_missing)\n",
        "else:\n",
        "    print(\"\\nNo missing values found in the dataset.\")\n",
        "complication_sum = df_no_duplicates[\"complication\"].sum()\n",
        "mort30_sum = df_no_duplicates[\"mort30\"].sum()\n",
        "\n",
        "print(\"Sum of complication:\", complication_sum)\n",
        "print(\"Sum of mort30:\", mort30_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ART-l024fxgW"
      },
      "source": [
        "## **Subtask 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IpVMeccarot"
      },
      "source": [
        "*Data-Quality Audit*\n",
        "\n",
        "Missingness Summary\n",
        "* The dataset is clean with respect to completeness, as there are 0 missing\n",
        "values across all 25 columns. Since each feature has a 100% fill rate, so imputation strategies are necessary.\n",
        "\n",
        "\n",
        "Duplicate Rows Check\n",
        "* Significant quantity of duplicates identified: 2902 rows (~19.8% of the original 14,635 entries).\n",
        "* These rows were resultantly removed, leaving a dataset of 11,733 unique rows.\n",
        "\n",
        "Target Distribution\n",
        "* Primary target (`complication`): There are 3690 positive cases out of 11,733. This represents a 31.4% complication rate. This is a relatively healthy distribution for a binary classifier given the size of the dataset.\n",
        "* Secondary Target (`mort30`): There are only 58 positive cases (~0.5%). This is highly imbalanced, and should likely be dropped.\n",
        "\n",
        "Ethical Considerations\n",
        "* The ethical considerations around this data set involve the classification of race. \n",
        "* The dataset only uses three different classifications of race: Caucasian, African American, and Other. This concerning as many classifications of race, if necessary for prediction, are grouped into \"Other,\" which we believe could make the model biased against the other group since there would be more noise surrounding those data points, rather than precisely identifying them uniquely. For that reason we consider dropping the race category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Subtask 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORS5NycAgrlj"
      },
      "source": [
        "Possible leakage vectors include `mort30` as the `mort30` column is an outcome after the surgery happened. Therefore if we are trying to predict if there was a complication in surgery, `mort30` is likely highly indicative of the complication and directly related to it. Additionally, it would not be data that we have access to before surgery, which is ideally when you would want to do this prediction/use this model. To prevent this vector, we drop the `mort30` column in order to not include this data in our prediction.\n",
        "\n",
        "Other risks of leakage are related to how the data is preprocessed. In order to prevent these, we should fit any of our preprocessing bits before we split the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Part B**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTVp37DtX5Bd"
      },
      "source": [
        "Since a year feature is not included, we do not have a true time-based ordering of the data, which is why we do not perform a time-based split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lMQ8ZX9_cnjG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression  # or any model\n",
        "\n",
        "# Drop unwanted columns\n",
        "df_clean = df_no_duplicates.drop(\n",
        "    columns=[\"mort30\", \"moonphase\", \"month\", \"dow\", \"hour\"],\n",
        "    errors=\"ignore\"\n",
        ")\n",
        "\n",
        "# Split features / target\n",
        "X = df_clean.drop(columns=[\"complication\"])\n",
        "y = df_clean[\"complication\"]\n",
        "\n",
        "# 80/20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3W8LPMmdtTD",
        "outputId": "7837c87c-f14b-4dc6-8b7c-cf41c65e0805"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.749893481039625"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_pipeline = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=71,\n",
        "        weights=\"distance\",\n",
        "        metric=\"euclidean\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit ONLY on training data\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "knn_pipeline.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpcPGouQeri5",
        "outputId": "60f10441-8e5e-4160-8f31-20290864a092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1512  100]\n",
            " [ 487  248]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = knn_pipeline.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGpI4uitiPlZ"
      },
      "source": [
        "We dropped the column `mort30` since it would be a form of data leakage, as it is only known after the fact. We dropped `moonphase`, `month`, `dow`, and `hour` to prevent overfitting. Our pipeline then involved a standard scaler, and then kNN classification using the Euclidean metric. We reported an accuracy of 74.99% which is not great because the class split is roughly 70/30 but it does show some promise as other testers' models received at best ~80% with more intricate models.\n",
        "\n",
        "The confusion matrix is:\n",
        "[[1512  100]\n",
        " [ 487  248]]\n",
        "\n",
        "* True Negatives (TN): 1512 – Cases where the model correctly predicted no complication, and there actually was none.\n",
        "* False Positives (FP): 100 – Cases where the model incorrectly predicted a complication, but there actually was none.\n",
        "* False Negatives (FN): 487 – Cases where the model incorrectly predicted no complication, but there actually was one.\n",
        "* True Positives (TP): 248 – Cases where the model correctly predicted a complication, and there actually was one."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
